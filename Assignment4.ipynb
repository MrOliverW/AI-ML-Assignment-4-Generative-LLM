{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4e844b-f31e-4fa2-9ff8-1726689cb6e6",
   "metadata": {},
   "source": [
    "Using the Hugging Face transforms library with PyTorch as the backend and other core libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4cc93b-d644-47fc-876e-185eec1a9819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\olive\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\olive\\anaconda3\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\olive\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\olive\\anaconda3\\lib\\site-packages (4.57.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: accelerate in c:\\users\\olive\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\olive\\anaconda3\\lib\\site-packages (0.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\olive\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\olive\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\olive\\anaconda3\\lib\\site-packages (2.20.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\olive\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\olive\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\olive\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\olive\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers accelerate bitsandbytes\n",
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce4f36-5bdc-4787-b204-a9d34579e5c6",
   "metadata": {},
   "source": [
    "Setting environment variable to avoid Keras 3/Transforms conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86deffce-bde1-4cf6-af67-b7fddfc0ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable set for Keras compatibility.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "print(\"Environment variable set for Keras compatibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12589ce4-0b5e-4beb-af58-623abaab8b54",
   "metadata": {},
   "source": [
    "Imports\n",
    "\n",
    "The Auto class automatically detects the specific architecture of the Mistral model and loads the correct corresponding PyTorch (or TensorFlow) model class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e287f-fe0b-4ca0-97e8-bafe2236e5e1",
   "metadata": {},
   "source": [
    "AutoTokenizer is the translator. It handles the essential process of converting human-readable text into numerical data (tokens/IDs) that the model can understand, and converting the model's numerical output back into readable text. The tokernizer breaks data down into numbers and assigns a unique ID to each piece. It then stiches those numberical IDs back together to form the final output.\n",
    "\n",
    "AutoTokenizer automatically detects the specific tokenizer associated with the Mistral model (which might use techniques like Byte-Pair Encoding or SentencePiece) and loads the correct version.\n",
    "\n",
    "Time is a standard Python library used to handle time-related tasks.\n",
    "\n",
    "The pipeline is a high-level wrapper that is often more stable in Jupyter environments than custom widget-heavy setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3240648f-7abe-49ec-b0f1-e3b2da089a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\olive\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c06d2-292a-4794-abc5-1723c007ab7b",
   "metadata": {},
   "source": [
    "Definings model and configuring for quantization\n",
    "\n",
    "This cell sets up the device, quantization, and loads the 7-billion-parameter Mistral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2dc2f75-6445-4efe-b025-57015ffcd64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 components directly onto cpu.\n",
      "\n",
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- REVISED Cell 2: Simplified Model Loading (No direct model object) ---\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {MODEL_ID} components directly onto {DEVICE}.\")\n",
    "\n",
    "try:\n",
    "    # Load Tokenizer ONLY\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    \n",
    "    # We will NOT load the 'model' object here, \n",
    "    # as the pipeline will handle that in the next step for better stability.\n",
    "\n",
    "    print(\"\\nTokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFATAL ERROR during tokenizer loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0f976-78c4-4866-a7cc-9afa165ce670",
   "metadata": {},
   "source": [
    "#Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f61b94-81ae-40f9-8399-070ebb9fd602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-Instruct-v0.2 directly onto cpu. This may take time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32bca5754634259ad0db5dcbc677923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- NEW Cell 2: Simplified Model Loading ---\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# NO MORE bnb_config definition here!\n",
    "\n",
    "print(f\"Loading {MODEL_ID} directly onto {DEVICE}. This may take time.\")\n",
    "\n",
    "try:\n",
    "    # Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "    # Load Model - NOTE: NO quantization_config or device_map=\"auto\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "    ).to(DEVICE) # <-- Explicitly move the final model to the target device\n",
    "\n",
    "    print(\"\\nModel and Tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFATAL ERROR during model loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9405d6-fd29-4eb6-8a4e-cdbe6dbd2f20",
   "metadata": {},
   "source": [
    "This cell defines the prompt and the single, corrected generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51847531-1642-4e24-b55c-0e94030a00de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing text generation pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27712f47e68b437f8b0623e4c92b65a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Generation Experiments (Varying Temperature) ---\n",
      "\n",
      "[Run 1/3] Generating story with Temperature = 0.2...\n",
      "\n",
      "[Run 2/3] Generating story with Temperature = 0.7...\n",
      "\n",
      "[Run 3/3] Generating story with Temperature = 1.0...\n",
      "\n",
      "--- Summary of Generated Outputs ---\n",
      "\n",
      "### Test Case 1: Temperature = 0.2 (206.17 seconds)\n",
      "--------------------------------------------------\n",
      "In the heart of the quaint, cobblestoned town of Briarwood, nestled between the labyrinthine lanes and the hallowed halls of history, resided a man of peculiar passions and unquenchable curiosity. His name was Edgar Montrose, a renowned antique clock collector, whose home was a veritable museum of time\n",
      "--------------------------------------------------\n",
      "\n",
      "### Test Case 2: Temperature = 0.7 (110.53 seconds)\n",
      "--------------------------------------------------\n",
      "In the heart of the bustling city of London, nestled amidst the labyrinthine network of cobblestone streets and quaint, ivy-covered houses, lies an unassuming antique shop, known to a select few as the \"Sanctum of Time.\" Its proprietor, Edgar Lockwood, was an esteemed collector of antique clocks,\n",
      "--------------------------------------------------\n",
      "\n",
      "### Test Case 3: Temperature = 1.0 (105.79 seconds)\n",
      "--------------------------------------------------\n",
      "In the quaint, cobblestone alleys of Old Town, where antique shops bloomed like ancient roses among the cobwebs of time, resided a man named Edgar. An ardent collector of ornate antique clocks, his home was a labyrinth of gears, weights, and pendulums ticking away in harmonious discord\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- REVISED Cell 3 & 4: Define Prompt, Pipeline, and Run Experiment ---\n",
    "\n",
    "# Define the Prompt\n",
    "CREATIVE_PROMPT = \"Write a short, suspenseful story about an antique clock collector who finds a cryptic message hidden inside a newly acquired timepiece from the 1800s. The message must hint at a long-lost treasure.\"\n",
    "\n",
    "# --- Initialize the Pipeline (Handles loading the model and moving it to the device) ---\n",
    "print(\"Initializing text generation pipeline...\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16 if DEVICE == \"cuda\" else None, \n",
    "    device=0 if DEVICE == \"cuda\" else -1 # 0 for GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "def generate_story_pipe(temperature: float, max_length: int = 80):\n",
    "    \"\"\"Generates text using the Hugging Face pipeline.\"\"\"\n",
    "    \n",
    "    # Format the prompt using the chat template\n",
    "    messages = [{\"role\": \"user\", \"content\": CREATIVE_PROMPT}]\n",
    "    prompt_formatted = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate the output sequence using the pipeline\n",
    "    output = pipe(\n",
    "        prompt_formatted,\n",
    "        max_new_tokens=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "        return_full_text=False # Only return the generated part\n",
    "    )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    output_text = output[0]['generated_text']\n",
    "    \n",
    "    return output_text, generation_time, temperature\n",
    "\n",
    "# --- STEP 4: Parameter Experimentation ---\n",
    "EXPERIMENT_RESULTS = []\n",
    "TEMPERATURE_VALUES = [0.2, 0.7, 1.0]\n",
    "EXPERIMENT_MAX_LENGTH = 80 # Keep it short for speed!\n",
    "\n",
    "print(\"\\n--- Starting Generation Experiments (Varying Temperature) ---\")\n",
    "\n",
    "for temp in TEMPERATURE_VALUES:\n",
    "    print(f\"\\n[Run {len(EXPERIMENT_RESULTS) + 1}/3] Generating story with Temperature = {temp}...\")\n",
    "    \n",
    "    # Call the new function\n",
    "    story_text, gen_time, current_temp = generate_story_pipe(\n",
    "        temperature=temp, \n",
    "        max_length=EXPERIMENT_MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    EXPERIMENT_RESULTS.append({\n",
    "        \"temperature\": current_temp,\n",
    "        \"time\": f\"{gen_time:.2f} seconds\",\n",
    "        \"story\": story_text.strip()\n",
    "    })\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Summary of Generated Outputs ---\")\n",
    "for i, result in enumerate(EXPERIMENT_RESULTS):\n",
    "    print(f\"\\n### Test Case {i+1}: Temperature = {result['temperature']} ({result['time']})\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(result['story'])\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a208975-82e7-48ee-a68c-4f20b6b5d54f",
   "metadata": {},
   "source": [
    "Executing the experiments and pritings results for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd629379-06ae-4a43-8dc3-84f955146eb1",
   "metadata": {},
   "source": [
    "Parameter expermintation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14cca7b-cd1a-4841-91c7-94977ee48874",
   "metadata": {},
   "source": [
    "The goal is to determine how the Temperature (a sampling parameter) influences the model's output quality. Temperature controls the randomness of the token selection:\n",
    "\n",
    "Low Temperature (0.2): High probability, less random, more predictable (safe, coherent).\n",
    "\n",
    "High Temperature (1.0): Low probability, highly random, more creative (unpredictable, potentially rambling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226ed05-174e-444e-9b54-129c7d7ce3e3",
   "metadata": {},
   "source": [
    "compare the stories you generated for Coherence, Creativity, and Adherence to the Prompt (CREATIVE_PROMPT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf6d49-a3b7-4af5-8e6f-71f1476ea0c5",
   "metadata": {},
   "source": [
    "0.2\tHighly structured, predictable, and formally written prose. Slowest generation.\t\n",
    "\n",
    "Best for formal summaries, factual answers, or generating text where accuracy and coherence are paramount.\n",
    "\n",
    "\n",
    "0.7\tBalanced creativity and quality. Fastest generation for high quality.\t\n",
    "\n",
    "Best for creative writing, ideation, and general-purpose answers where you want some novelty but need to stay on-topic.\n",
    "\n",
    "\n",
    "1.0\tHighly novel, poetic language, and less predictable narrative choices. Fast generation.\t\n",
    "\n",
    "Best for brainstorming, generating dialogue, or forcing the model to break away from common phrases and provide maximum variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619e709-112d-4e46-9b29-f6212915be22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e5b32-d91e-46ad-b3de-c9163b87cc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80561c-49bd-40a3-8ba1-cc03cc047553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e707a3-a496-4404-86ad-8f5ab679692f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e04e8a-7b35-4d85-994e-a7547e65cf71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
